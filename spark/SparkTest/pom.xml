<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>

	<groupId>cn.edu360.spark</groupId>
	<artifactId>SparkTest</artifactId>
	<version>1.0-SNAPSHOT</version>

	<properties>
		<maven.compiler.source>1.8</maven.compiler.source>
		<maven.compiler.target>1.8</maven.compiler.target>
		<scala.version>2.11.8</scala.version>
		<!--<spark.version>2.2.0</spark.version>-->
		<spark.version>2.4.3</spark.version>
		<hadoop.version>2.8.1</hadoop.version>
		<encoding>UTF-8</encoding>
	</properties>
	<dependencies>
		<!-- 导入scala的依赖 -->
		<dependency>
			<groupId>org.scala-lang</groupId>
			<artifactId>scala-library</artifactId>
			<version>${scala.version}</version>
		</dependency>

		<!-- 导入spark的依赖 -->
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-core_2.11</artifactId>
			<version>${spark.version}</version>
		</dependency>

		<!-- 指定hadoop-client API的版本 -->
		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-client</artifactId>
			<version>${hadoop.version}</version>
		</dependency>

		<!-- 导入spark sql的依赖 -->
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-sql_2.11</artifactId>
			<version>${spark.version}</version>
		</dependency>

		<!-- spark如果想整合Hive，必须加入hive的支持 -->
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-hive_2.11</artifactId>
			<version>${spark.version}</version>
		</dependency>

		<!-- spark steaming的依赖 -->
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-streaming_2.11</artifactId>
			<version>2.2.0</version>
		</dependency>

		<!-- sparkSteaming跟Kafka整合的依赖 -->
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-streaming-kafka-0-10_2.11</artifactId>
			<version>${spark.version}</version>
		</dependency>


		<!-- mysql的连接驱动依赖 -->
		<dependency>
			<groupId>mysql</groupId>
			<artifactId>mysql-connector-java</artifactId>
			<version>5.1.38</version>
		</dependency>


		<dependency>
			<groupId>redis.clients</groupId>
			<artifactId>jedis</artifactId>
			<version>2.9.0</version>
		</dependency>
		<dependency>
			<groupId>mysql</groupId>
			<artifactId>mysql-connector-java</artifactId>
			<version>8.0.17</version>
			<scope>compile</scope>
		</dependency>

		<!--读取excel文件 -->
		<!--<dependency>
			<groupId>org.apache.poi</groupId>
			<artifactId>poi</artifactId>
			<version>3.15</version>
		</dependency>-->
		<!--<dependency>
			<groupId>org.apache.poi</groupId>
			<artifactId>poi-ooxml</artifactId>
			<version>3.15</version>
		</dependency>-->
	<!--	<dependency>
			<groupId>com.crealytics</groupId>
			<artifactId>spark-excel_2.12</artifactId>
			<version>0.12.5</version>
		</dependency>-->
		<dependency>
			<groupId>com.databricks</groupId>
			<artifactId>spark-csv_2.11</artifactId>
			<version>1.4.0</version>
		</dependency>
		<dependency>
			<groupId>com.crealytics</groupId>
			<artifactId>spark-excel_2.11</artifactId>
			<version>0.13.1</version>
		</dependency>
		<dependency>
			<groupId>com.databricks</groupId>
			<artifactId>spark-xml_2.11</artifactId>
			<version>0.6.0</version>
		</dependency>
        <dependency>
            <groupId>ch.qos.logback</groupId>
            <artifactId>logback-core</artifactId>
            <version>1.2.3</version>
        </dependency>
		<dependency>
			<groupId>ch.qos.logback</groupId>
			<artifactId>logback-core</artifactId>
			<version>1.2.3</version>
		</dependency>
		<dependency>
			<groupId>commons-codec</groupId>
			<artifactId>commons-codec</artifactId>
			<version>1.12</version>
			<scope>compile</scope>
		</dependency>
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <version>1.18.8</version>
            <scope>compile</scope>
        </dependency>
		<dependency>
			<groupId>org.apache.spark</groupId>
			<artifactId>spark-mllib_2.11</artifactId>
			<version>${spark.version}</version>
		</dependency>
        <dependency>
            <groupId>com.mchange</groupId>
            <artifactId>c3p0</artifactId>
            <version>0.9.5.2</version>
            <scope>compile</scope>
        </dependency>
		<dependency>
			<groupId>com.alibaba</groupId>
			<artifactId>fastjson</artifactId>
			<version>1.2.32</version>
		</dependency>
		<!--分词组件-->
		<dependency>
			<groupId>org.apdplat</groupId>
			<artifactId>word</artifactId>
			<version>1.2</version>
		</dependency>
		<dependency>
			<groupId>org.jsoup</groupId>
			<artifactId>jsoup</artifactId>
			<version>1.11.3</version>
		</dependency>
		<dependency>
			<groupId>org.pentaho</groupId>
			<artifactId>pentaho-aggdesigner-algorithm</artifactId>
			<version>5.1.5-jhyde</version>
			<scope>test</scope>
		</dependency>
    </dependencies>

	<build>
		<finalName>SprakDemo</finalName>
		<plugins>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-assembly-plugin</artifactId>
				<version>2.4.1</version>
				<configuration>
					<appendAssemblyId>false</appendAssemblyId>
					<finalName>cleandata</finalName>
					<descriptorRefs>
						<!-- 将依赖的jar包中的class文件打进生成的jar包 -->
						<descriptorRef>jar-with-dependencies</descriptorRef>
					</descriptorRefs>
					<archive>
						<manifest>
							<addClasspath>true</addClasspath>
							<!-- 有Main函数的类 -->
							<mainClass>com.hadoop.CleanData</mainClass>
						</manifest>
					</archive>
				</configuration>
				<executions>
					<execution>
						<id>make-assembly</id>
						<phase>package</phase>
						<goals>
							<!-- <goal>single</goal> -->
							<goal>assembly</goal>
						</goals>
					</execution>
				</executions>
			</plugin>
		</plugins>
	</build>
</project>